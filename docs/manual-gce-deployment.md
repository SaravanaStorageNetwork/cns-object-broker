# Deploying Gluster-Kubernetes, S3, and CNS Object Broker Manually in GCE

## Limitations

- Kubernetes is deployed via the `kubeadm` tool.  This is a work in progress deployer with its own [limitations](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations).

## Assumptions
- A [Google Cloud Platform](https://cloud.google.com/) account.
- 4 Running RHEL 7.2+ GCE Instances.
- Each minion instance has an additional raw block device.  These devices must be identical in size across the cluster.
- The kubernetes network overlay will be handled by Flannel. (Other network overlays are [available](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network)).

## Package Dependencies
- All Nodes:

  `docker-1.12.6`
  `glusterfs-fuse`
  `kubectl`
  `kubeadm`

- Master Node Only:

  `git`
  `helm`
  `socat`
  [`cns-object-broker`](https://github.com/copejon/cns-object-broker/)
  [`gluster-kubernetes`](https://github.com/gluster/gluster-kubernetes)
  [`heketi-cli`](https://github.com/heketi/heketi/releases/tag/v4.0.0)

---

## Installation

The installation process is automated by
[gk-cluster-deploy/vm/do-startup.sh](https://github.com/copejon/gk-cluster-deploy/blob/master/deploy/vm/do-startup.sh).
If you do not want to deploy each node manually, you can run this script on each VM.  **The script must be run as `root`.**  It will determine if it is on the *master* node from the hostname of the node.

## Step by Step Deployment

**NOTE: All actions are meant to be executed as `root` user.**

### Laying the Foundation

#### On All Nodes...

1. Stop and Disable `firewalld`

    `# systemctl stop firewalld && systemctl disable firewalld`

2. Create the kubernetes yum repo

    ```
    su -c "cat <<EOF > /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
    enabled=1
    gpgcheck=1
    repo_gpgcheck=1
    gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
    https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
    EOF"
    ```

3. Yum Install RPMs

    `# yum install -y docker-1.12.6 glusterfs-fuse kubelet kubeadm`

4. Enable and Start Docker

    `# systemctl enable docker && systemctl start docker`

4. Install `kubectl`

  - Get the binary

    `# curl -sSLO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl`

  - Make the binary executable

    `# chmod +x ./kubectl`

  - Move the binary to `bin` dir

    `# mv ./kubectl /usr/bin/`


#### On the Master Node

1. Install Master RPMs

    `# yum install -y git socat`

2. Clone [CNS Object Broker](https://github.com/copejon/cns-object-broker)

    `# git clone https://github.com/copejon/cns-object-broker.git`

3. Clone [Gluster-Kubernetes](https://github.com/gluster/gluster-kubernetes)

    `# git clone https://github.com/gluster/gluster-kubernetes.git`

4. Install `helm` binary

  - Get the binary

    `# curl -sSL https://storage.googleapis.com/kubernetes-helm/helm-v2.5.0-linux-amd64.tar.gz | tar zx`

  - Move the binary to `bin` dir

    `# mv $(find /tmp/ -name helm) /usr/bin/`

5. Install `heketi-cli`

  - Get the binary

    `# curl -sSL https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-client-v4.0.0.linux.amd64.tar.gz | tar -xz`

  - Move the binary to `bin` dir

    `# mv $(find ./ -name heketi-cli) /usr/bin/`

6. Initialize the kubernetes master node.

    The following command initializes the master node *and* Flannel as the pod network.
    If you would prefer to use a different pod network, see the [kubernetes documentation](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network) for other options.

    `# kubeadm init --pod-network-cidr=10.244.0.0/16`

    **Note the `kubeadm join` command output by `kubeadm init`!  This is used to connect minions to the master.**

#### Initialize and Connect Minions to the Master

**NOTE: This requires the `kubeadm join` command output by `kubeadm init`.**

0. If you no longer have the `kubeadm join` command, you can retrieve the token.  On the master, execute

    `# kubeadm token list`

    Which should output

    ```
    TOKEN     <...>   DESCRIPTION        
    <token>   <...>   The default bootstrap token generated by 'kubeadm init'.
    ```

    Save the token value for the next step.

1. On **each** minion, execute the `kubeadm join` command:

    `# kubeadm join --token <token> <master internal ip>:6443`

    **NOTE: in `kubeadm join`, the option `--discovery-token-ca-cert-hash <ca cert token>` is not required.**

    Should output
    ```
    [kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
    [preflight] Running pre-flight checks
    [preflight] Starting the kubelet service
    [validation] WARNING: using token-based discovery without DiscoveryTokenCACertHashes can be unsafe (see https://kubernetes.io/docs/admin/kubeadm/#kubeadm-join).
    [validation] WARNING: Pass --discovery-token-unsafe-skip-ca-verification to disable this warning. This warning will become an error in Kubernetes 1.9.
    [discovery] Trying to connect to API Server "10.128.0.5:6443"
    [discovery] Created cluster-info discovery client, requesting info from "https://10.128.0.5:6443"
    [discovery] Cluster info signature and contents are valid and no TLS pinning was specified, will use API Server "10.128.0.5:6443"
    [discovery] Successfully established connection with API Server "10.128.0.5:6443"
    [bootstrap] Detected server version: v1.8.0
    [bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)

    Node join complete:
    * Certificate signing request sent to master and response
      received.
    * Kubelet informed of new secure connection details.

    Run 'kubectl get nodes' on the master to see this machine join.
    ```

2. Once all minions are attached, check their status on the **master** node

    `# kubectl get nodes`

    Should output

    ```
    NAME                 STATUS                     ROLES     AGE       VERSION
    <master>             Ready,SchedulingDisabled   master    1h        v1.8.0
    <minion1>            Ready                      <none>    1h        v1.8.0
    <minion2>            Ready                      <none>    1h        v1.8.0
    <minion3>            Ready                      <none>    1h        v1.8.0
    ```

---

## Deploying Gluster-Kubernetes

**NOTE: all actions in this section are performed on the master GCE node as `root`.**

### Define GK Topology

1. Change directory to `gluster-kubernetes/deploy`

2. Make a copy of the `topology.json.sample` to edit

    `# cp topology.json.sample topology.json`

3. In `topology.json`, each node must be identified by its *hostname* and *internal IP*.
The absolute path (e.g. `/dev/sdb`) of the additional raw block device must be provided under `devices`.

    Define each `node` block to reference a different minion in the cluster.  The topology file must have *at least* 3 nodes referenced.
    More can be added if there are more minions deployed.

    ```json
    {
      "node": {
        "hostnames": {
          "manage": [
            "<HOSTNAME>"
          ],
          "storage": [
            "<INTERNAL IP>"
          ]
        },
        "zone": 1
        },
        "devices": [
          "/dev/<raw block device e.g. /dev/sdb>"
        ]
    },
    ```

### Run `gk-deploy.sh`

1.  After the topology file is properly configured, it is time to deploy gluster-kubernetes.  In `gluster-kubernetes/deploy`, execute

    `# ./gk-deploy.sh -gvy --no-block --object-account=<account name> --object-user=<user name> --object-password=<password> topology.json`

    Each `--object-*` option is required to setup the S3 interface.  Keep the value simple.  It is okay to use the same value for each option.

    - Option Breakdown

      - `-g` triggers glusterfs deployment
      - `-v` verbose install
      - `-y` skips pre-install checklist
      - `--no-block` prevents install of glusterfs block storage components, not needed in the demo
      - `--object-*` are required to trigger install of glusterfs object storage components

    - The script can take up to 5 minutes run.

2. Once the script completes, verify the component pods are running

    `# kubectl get pods`

    Should output

    ```
    NAME                                     READY     STATUS    RESTARTS   AGE
    gluster-s3-deployment-7d455cf89f-zzdcq   1/1       Running   0          44m
    glusterfs-c5vws                          1/1       Running   0          47m
    glusterfs-hwvxq                          1/1       Running   0          47m
    glusterfs-tk4dv                          1/1       Running   0          47m
    heketi-6895d8ccbc-gtrgj                  1/1       Running   0          45m
    ```

3. Verify component services are Running

    `# kubectl get svc`

    Should output

    ```
    NAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    gluster-s3-service                        ClusterIP   <cluster ip>     <none>        8080/TCP         48m
    glusterfs-dynamic-gluster-s3-claim        ClusterIP   <cluster ip>     <none>        1/TCP            48m
    glusterfs-dynamic-gluster-s3-meta-claim   ClusterIP   <cluster ip>     <none>        1/TCP            48m
    heketi                                    ClusterIP   <cluster ip>     <none>        8080/TCP         49m
    heketi-storage-endpoints                  ClusterIP   <cluster ip>     <none>        1/TCP            4
    ```

### Expose the `gluster-s3-deployment` Pod

1. To make the S3 interface reachable from the outside world, expose it using a `NodePort` service:

    `# kubectl expose deployment gluster-s3-deployment --type=NodePort --port=8080`

2.  Verify the port

    `# kubectl get svc gluster-s3-deployment`

    Should output
    ```
    NAME                    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                     AGE
    gluster-s3-deployment   NodePort   <cluster ip>     <none>        8080:<external port>/TCP    53m
    ```

---

## Deploying the CNS Object Broker

### Setup Helm

1. Setup cluster admin role binding. This is required by `helm`.

    `# kubectl create clusterrolebinding --clusterrole=cluster-admin --serviceaccount=kube-system:default cluster-addon`

2. Initialize `helm`

  - Deploy the cluster side component of `helm`, called Tiller.

    `# helm init`

    Should output
    ```
    $HELM_HOME has been configured at /root/.helm.

    Tiller (the helm server side component) has been installed into your Kubernetes Cluster.
    Happy Helming!
    ```

  - Ensure Tiller is Running

    `# kubectl get pods -n kube-system | grep tiller`

    Should output

    ```
    tiller-deploy-554694466b-tqlrr            1/1       Running   0          1h
    ```

### Install the CNS Object broker

1. From the `cns-object-broker/` directory, execute

    `# helm install chart/ --name broker --namespace broker`

    Should output

    ```
    NAME:   broker
    LAST DEPLOYED: Mon Oct  2 20:28:36 2017
    NAMESPACE: broker
    STATUS: DEPLOYED

    RESOURCES:
    ==> v1/Service
    NAME                                CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE
    broker-cns-object-broker-node-port  10.109.165.178  <nodes>      8080:31128/TCP  1s
    broker-cns-object-broker            10.111.113.240  <none>       80/TCP          0s

    ==> v1beta1/Deployment
    NAME                      DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
    broker-cns-object-broker  1        1        1           0          0s

    ==> v1beta1/RoleBinding
    NAME            AGE
    cns-obj-broker  0s

    ==> v1beta1/Role
    NAME            AGE
    cns-obj-broker  0s
    ```

2. Verify the broker is Running

    `# kubectl -n broker get pod,svc`

    Should output

    ```
    NAME                                          READY     STATUS    RESTARTS   AGE
    po/broker-cns-object-broker-d85d8bc5b-jx4ss   1/1       Running   0          1m

    NAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    svc/broker-cns-object-broker             ClusterIP   10.111.113.240   <none>        80/TCP           1m
    svc/broker-cns-object-broker-node-port   NodePort    10.109.165.178   <none>        8080:31128/TCP   1m
    ```

3.  Note the external port of `svc/broker-cns-object-broker-node-port`.  Under ports, the format is `<pod port>:<external port>/TCP`.
This is required for the service broker api object definition that we will create on the local *all-in-one* kubernetes cluster.

**Return to the [Main Page](../README.md#step-2-deploy-the-service-catalog) to continue with the Service Catalog deployment.**
